{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Deepak-Mewada/Miscellaneous/blob/main/US_diesel_prices.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Lka8Wr9BJIH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15842d62-6544-42f8-8964-86b155ab55b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d (Conv1D)             (32, 8, 32)               128       \n",
            "                                                                 \n",
            " depthwise_conv1d (Depthwise  (32, 6, 128)             512       \n",
            " Conv1D)                                                         \n",
            "                                                                 \n",
            " flatten (Flatten)           (32, 768)                 0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (32, 768)                 0         \n",
            "                                                                 \n",
            " dense (Dense)               (32, 320)                 246080    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (32, 320)                 0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (32, 160)                 51360     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (32, 80)                  12880     \n",
            "                                                                 \n",
            " dense_3 (Dense)             (32, 40)                  3240      \n",
            "                                                                 \n",
            " dense_4 (Dense)             (32, 20)                  820       \n",
            "                                                                 \n",
            " dense_5 (Dense)             (32, 10)                  210       \n",
            "                                                                 \n",
            " out_layer (OutLayer)        (32, 10, 1)               0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 315,230\n",
            "Trainable params: 315,230\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/250\n",
            "35/35 [==============================] - 1s 19ms/step - loss: 0.0549 - val_loss: 0.0532\n",
            "Epoch 2/250\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0884 - val_loss: 0.0835\n",
            "Epoch 3/250\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.1234 - val_loss: 0.1995\n",
            "Epoch 4/250\n",
            "35/35 [==============================] - 0s 13ms/step - loss: 0.1789 - val_loss: 0.1297\n",
            "Epoch 5/250\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.1282 - val_loss: 0.0630\n",
            "Epoch 6/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.1319 - val_loss: 0.1060\n",
            "Epoch 7/250\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.1364 - val_loss: 0.1013\n",
            "Epoch 8/250\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.1342 - val_loss: 0.1073\n",
            "Epoch 9/250\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.1313 - val_loss: 0.0824\n",
            "Epoch 10/250\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.1608 - val_loss: 0.1641\n",
            "Epoch 11/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.1523 - val_loss: 0.1446\n",
            "Epoch 12/250\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.1530 - val_loss: 0.1123\n",
            "Epoch 13/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.1223 - val_loss: 0.0987\n",
            "Epoch 14/250\n",
            "35/35 [==============================] - 1s 17ms/step - loss: 0.1541 - val_loss: 0.2502\n",
            "Epoch 15/250\n",
            "35/35 [==============================] - 1s 16ms/step - loss: 0.1607 - val_loss: 0.1042\n",
            "Epoch 16/250\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.1952 - val_loss: 0.1712\n",
            "Epoch 17/250\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.1455 - val_loss: 0.1718\n",
            "Epoch 18/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.1892 - val_loss: 0.0738\n",
            "Epoch 19/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0923 - val_loss: 0.0428\n",
            "Epoch 20/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0475 - val_loss: 0.0276\n",
            "Epoch 21/250\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.0775 - val_loss: 0.0447\n",
            "Epoch 22/250\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.2057 - val_loss: 0.1217\n",
            "Epoch 23/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0615 - val_loss: 0.0401\n",
            "Epoch 24/250\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.1177 - val_loss: 0.1268\n",
            "Epoch 25/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.1171 - val_loss: 0.0342\n",
            "Epoch 26/250\n",
            "35/35 [==============================] - 0s 13ms/step - loss: 0.0634 - val_loss: 0.0468\n",
            "Epoch 27/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0527 - val_loss: 0.0308\n",
            "Epoch 28/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0588 - val_loss: 0.0359\n",
            "Epoch 29/250\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.1445 - val_loss: 0.0707\n",
            "Epoch 30/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0484 - val_loss: 0.0341\n",
            "Epoch 31/250\n",
            "35/35 [==============================] - 0s 13ms/step - loss: 0.0476 - val_loss: 0.0297\n",
            "Epoch 32/250\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0923 - val_loss: 0.0469\n",
            "Epoch 33/250\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.1001 - val_loss: 0.0740\n",
            "Epoch 34/250\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0543 - val_loss: 0.0329\n",
            "Epoch 35/250\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0648 - val_loss: 0.0319\n",
            "Epoch 36/250\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.1191 - val_loss: 0.0486\n",
            "Epoch 37/250\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0507 - val_loss: 0.0299\n",
            "Epoch 38/250\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.0776 - val_loss: 0.0357\n",
            "Epoch 39/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0710 - val_loss: 0.0422\n",
            "Epoch 40/250\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.0710 - val_loss: 0.0381\n",
            "Epoch 41/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0617 - val_loss: 0.0285\n",
            "Epoch 42/250\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.0627 - val_loss: 0.0318\n",
            "Epoch 43/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0587 - val_loss: 0.0356\n",
            "Epoch 44/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0656 - val_loss: 0.0360\n",
            "Epoch 45/250\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.0530 - val_loss: 0.0295\n",
            "Epoch 46/250\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0572 - val_loss: 0.0259\n",
            "Epoch 47/250\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.0480 - val_loss: 0.0286\n",
            "Epoch 48/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0689 - val_loss: 0.0424\n",
            "Epoch 49/250\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.0512 - val_loss: 0.0276\n",
            "Epoch 50/250\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0636 - val_loss: 0.0351\n",
            "Epoch 51/250\n",
            "35/35 [==============================] - 1s 17ms/step - loss: 0.0447 - val_loss: 0.0267\n",
            "Epoch 52/250\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0731 - val_loss: 0.0569\n",
            "Epoch 53/250\n",
            "35/35 [==============================] - 1s 16ms/step - loss: 0.0588 - val_loss: 0.0345\n",
            "Epoch 54/250\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0619 - val_loss: 0.0388\n",
            "Epoch 55/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0375 - val_loss: 0.0277\n",
            "Epoch 56/250\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.0577 - val_loss: 0.0355\n",
            "Epoch 57/250\n",
            "35/35 [==============================] - 0s 13ms/step - loss: 0.0484 - val_loss: 0.0273\n",
            "Epoch 58/250\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0530 - val_loss: 0.0283\n",
            "Epoch 59/250\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.0435 - val_loss: 0.0248\n",
            "Epoch 60/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0543 - val_loss: 0.0357\n",
            "Epoch 61/250\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0432 - val_loss: 0.0298\n",
            "Epoch 62/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0537 - val_loss: 0.0376\n",
            "Epoch 63/250\n",
            "35/35 [==============================] - 1s 16ms/step - loss: 0.0452 - val_loss: 0.0260\n",
            "Epoch 64/250\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0453 - val_loss: 0.0317\n",
            "Epoch 65/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0440 - val_loss: 0.0254\n",
            "Epoch 66/250\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0535 - val_loss: 0.0300\n",
            "Epoch 67/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0378 - val_loss: 0.0253\n",
            "Epoch 68/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0552 - val_loss: 0.0381\n",
            "Epoch 69/250\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.0415 - val_loss: 0.0257\n",
            "Epoch 70/250\n",
            "35/35 [==============================] - 1s 16ms/step - loss: 0.0552 - val_loss: 0.0277\n",
            "Epoch 71/250\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.0380 - val_loss: 0.0325\n",
            "Epoch 72/250\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.0606 - val_loss: 0.0303\n",
            "Epoch 73/250\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.0384 - val_loss: 0.0289\n",
            "Epoch 74/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0529 - val_loss: 0.0261\n",
            "Epoch 75/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0422 - val_loss: 0.0271\n",
            "Epoch 76/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0520 - val_loss: 0.0256\n",
            "Epoch 77/250\n",
            "35/35 [==============================] - 0s 13ms/step - loss: 0.0405 - val_loss: 0.0302\n",
            "Epoch 78/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0559 - val_loss: 0.0344\n",
            "Epoch 79/250\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0399 - val_loss: 0.0239\n",
            "Epoch 80/250\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.0528 - val_loss: 0.0278\n",
            "Epoch 81/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0395 - val_loss: 0.0264\n",
            "Epoch 82/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0515 - val_loss: 0.0277\n",
            "Epoch 83/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0399 - val_loss: 0.0252\n",
            "Epoch 84/250\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.0502 - val_loss: 0.0291\n",
            "Epoch 85/250\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.0420 - val_loss: 0.0234\n",
            "Epoch 86/250\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.0467 - val_loss: 0.0387\n",
            "Epoch 87/250\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.0497 - val_loss: 0.0291\n",
            "Epoch 88/250\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0505 - val_loss: 0.0250\n",
            "Epoch 89/250\n",
            "35/35 [==============================] - 0s 13ms/step - loss: 0.0476 - val_loss: 0.0265\n",
            "Epoch 90/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0517 - val_loss: 0.0264\n",
            "Epoch 91/250\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.0490 - val_loss: 0.0285\n",
            "Epoch 92/250\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.0457 - val_loss: 0.0267\n",
            "Epoch 93/250\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.0529 - val_loss: 0.0273\n",
            "Epoch 94/250\n",
            "35/35 [==============================] - 1s 17ms/step - loss: 0.0473 - val_loss: 0.0236\n",
            "Epoch 95/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0465 - val_loss: 0.0283\n",
            "Epoch 96/250\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.0543 - val_loss: 0.0238\n",
            "Epoch 97/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0567 - val_loss: 0.0344\n",
            "Epoch 98/250\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0550 - val_loss: 0.0291\n",
            "Epoch 99/250\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.0515 - val_loss: 0.0257\n",
            "Epoch 100/250\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.0476 - val_loss: 0.0317\n",
            "Epoch 101/250\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0528 - val_loss: 0.0297\n",
            "Epoch 102/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0441 - val_loss: 0.0258\n",
            "Epoch 103/250\n",
            "35/35 [==============================] - 0s 13ms/step - loss: 0.0433 - val_loss: 0.0267\n",
            "Epoch 104/250\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0463 - val_loss: 0.0239\n",
            "Epoch 105/250\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.0458 - val_loss: 0.0304\n",
            "Epoch 106/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0521 - val_loss: 0.0259\n",
            "Epoch 107/250\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0405 - val_loss: 0.0304\n",
            "Epoch 108/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0560 - val_loss: 0.0303\n",
            "Epoch 109/250\n",
            "35/35 [==============================] - 1s 16ms/step - loss: 0.0369 - val_loss: 0.0276\n",
            "Epoch 110/250\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.0515 - val_loss: 0.0409\n",
            "Epoch 111/250\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0396 - val_loss: 0.0258\n",
            "Epoch 112/250\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.0489 - val_loss: 0.0287\n",
            "Epoch 113/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0363 - val_loss: 0.0256\n",
            "Epoch 114/250\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0472 - val_loss: 0.0422\n",
            "Epoch 115/250\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.0386 - val_loss: 0.0254\n",
            "Epoch 116/250\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.0488 - val_loss: 0.0307\n",
            "Epoch 117/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0361 - val_loss: 0.0226\n",
            "Epoch 118/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0425 - val_loss: 0.0291\n",
            "Epoch 119/250\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0401 - val_loss: 0.0261\n",
            "Epoch 120/250\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0448 - val_loss: 0.0290\n",
            "Epoch 121/250\n",
            "35/35 [==============================] - 0s 13ms/step - loss: 0.0395 - val_loss: 0.0240\n",
            "Epoch 122/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0404 - val_loss: 0.0316\n",
            "Epoch 123/250\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.0403 - val_loss: 0.0236\n",
            "Epoch 124/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0394 - val_loss: 0.0266\n",
            "Epoch 125/250\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0429 - val_loss: 0.0251\n",
            "Epoch 126/250\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0416 - val_loss: 0.0258\n",
            "Epoch 127/250\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.0442 - val_loss: 0.0250\n",
            "Epoch 128/250\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0403 - val_loss: 0.0250\n",
            "Epoch 129/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0473 - val_loss: 0.0234\n",
            "Epoch 130/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0396 - val_loss: 0.0250\n",
            "Epoch 131/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0456 - val_loss: 0.0247\n",
            "Epoch 132/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0420 - val_loss: 0.0222\n",
            "Epoch 133/250\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.0444 - val_loss: 0.0261\n",
            "Epoch 134/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0404 - val_loss: 0.0235\n",
            "Epoch 135/250\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.0433 - val_loss: 0.0291\n",
            "Epoch 136/250\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0465 - val_loss: 0.0377\n",
            "Epoch 137/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0585 - val_loss: 0.0370\n",
            "Epoch 138/250\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0535 - val_loss: 0.0451\n",
            "Epoch 139/250\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.0486 - val_loss: 0.0291\n",
            "Epoch 140/250\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0408 - val_loss: 0.0260\n",
            "Epoch 141/250\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0435 - val_loss: 0.0250\n",
            "Epoch 142/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0466 - val_loss: 0.0260\n",
            "Epoch 143/250\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0453 - val_loss: 0.0286\n",
            "Epoch 144/250\n",
            "35/35 [==============================] - 1s 26ms/step - loss: 0.0492 - val_loss: 0.0359\n",
            "Epoch 145/250\n",
            "35/35 [==============================] - 1s 21ms/step - loss: 0.0542 - val_loss: 0.0509\n",
            "Epoch 146/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0505 - val_loss: 0.0318\n",
            "Epoch 147/250\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0487 - val_loss: 0.0285\n",
            "Epoch 148/250\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0464 - val_loss: 0.0274\n",
            "Epoch 149/250\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0427 - val_loss: 0.0233\n",
            "Epoch 150/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0397 - val_loss: 0.0240\n",
            "Epoch 151/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0452 - val_loss: 0.0325\n",
            "Epoch 152/250\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0460 - val_loss: 0.0292\n",
            "Epoch 153/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0486 - val_loss: 0.0271\n",
            "Epoch 154/250\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.0478 - val_loss: 0.0457\n",
            "Epoch 155/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0532 - val_loss: 0.0543\n",
            "Epoch 156/250\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0474 - val_loss: 0.0235\n",
            "Epoch 157/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0443 - val_loss: 0.0303\n",
            "Epoch 158/250\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.0493 - val_loss: 0.0247\n",
            "Epoch 159/250\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0428 - val_loss: 0.0257\n",
            "Epoch 160/250\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0446 - val_loss: 0.0235\n",
            "Epoch 161/250\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0407 - val_loss: 0.0226\n",
            "Epoch 162/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0394 - val_loss: 0.0309\n",
            "Epoch 163/250\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0444 - val_loss: 0.0250\n",
            "Epoch 164/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0382 - val_loss: 0.0294\n",
            "Epoch 165/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0477 - val_loss: 0.0368\n",
            "Epoch 166/250\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0428 - val_loss: 0.0224\n",
            "Epoch 167/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0376 - val_loss: 0.0242\n",
            "Epoch 168/250\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0453 - val_loss: 0.0334\n",
            "Epoch 169/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0369 - val_loss: 0.0264\n",
            "Epoch 170/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0458 - val_loss: 0.0325\n",
            "Epoch 171/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0368 - val_loss: 0.0228\n",
            "Epoch 172/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0403 - val_loss: 0.0298\n",
            "Epoch 173/250\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0363 - val_loss: 0.0233\n",
            "Epoch 174/250\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0420 - val_loss: 0.0312\n",
            "Epoch 175/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0366 - val_loss: 0.0247\n",
            "Epoch 176/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0441 - val_loss: 0.0321\n",
            "Epoch 177/250\n",
            "35/35 [==============================] - 1s 16ms/step - loss: 0.0346 - val_loss: 0.0223\n",
            "Epoch 178/250\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.0408 - val_loss: 0.0284\n",
            "Epoch 179/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0380 - val_loss: 0.0225\n",
            "Epoch 180/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0397 - val_loss: 0.0297\n",
            "Epoch 181/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0419 - val_loss: 0.0266\n",
            "Epoch 182/250\n",
            "35/35 [==============================] - 1s 18ms/step - loss: 0.0391 - val_loss: 0.0241\n",
            "Epoch 183/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0384 - val_loss: 0.0229\n",
            "Epoch 184/250\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0393 - val_loss: 0.0214\n",
            "Epoch 185/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0382 - val_loss: 0.0221\n",
            "Epoch 186/250\n",
            "35/35 [==============================] - 1s 16ms/step - loss: 0.0412 - val_loss: 0.0211\n",
            "Epoch 187/250\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0384 - val_loss: 0.0229\n",
            "Epoch 188/250\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0422 - val_loss: 0.0208\n",
            "Epoch 189/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0376 - val_loss: 0.0225\n",
            "Epoch 190/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0422 - val_loss: 0.0309\n",
            "Epoch 191/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0373 - val_loss: 0.0278\n",
            "Epoch 192/250\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.0421 - val_loss: 0.0209\n",
            "Epoch 193/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0333 - val_loss: 0.0224\n",
            "Epoch 194/250\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0405 - val_loss: 0.0216\n",
            "Epoch 195/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0347 - val_loss: 0.0241\n",
            "Epoch 196/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0381 - val_loss: 0.0261\n",
            "Epoch 197/250\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.0395 - val_loss: 0.0230\n",
            "Epoch 198/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0387 - val_loss: 0.0269\n",
            "Epoch 199/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0411 - val_loss: 0.0237\n",
            "Epoch 200/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0401 - val_loss: 0.0209\n",
            "Epoch 201/250\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0404 - val_loss: 0.0219\n",
            "Epoch 202/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0428 - val_loss: 0.0246\n",
            "Epoch 203/250\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0412 - val_loss: 0.0226\n",
            "Epoch 204/250\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.0459 - val_loss: 0.0283\n",
            "Epoch 205/250\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0576 - val_loss: 0.0379\n",
            "Epoch 206/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0580 - val_loss: 0.0514\n",
            "Epoch 207/250\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0551 - val_loss: 0.0310\n",
            "Epoch 208/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0407 - val_loss: 0.0242\n",
            "Epoch 209/250\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.0398 - val_loss: 0.0261\n",
            "Epoch 210/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0403 - val_loss: 0.0292\n",
            "Epoch 211/250\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.0484 - val_loss: 0.0269\n",
            "Epoch 212/250\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0470 - val_loss: 0.0233\n",
            "Epoch 213/250\n",
            "35/35 [==============================] - 1s 17ms/step - loss: 0.0457 - val_loss: 0.0214\n",
            "Epoch 214/250\n",
            "35/35 [==============================] - 1s 20ms/step - loss: 0.0390 - val_loss: 0.0210\n",
            "Epoch 215/250\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0406 - val_loss: 0.0221\n",
            "Epoch 216/250\n",
            "35/35 [==============================] - 1s 26ms/step - loss: 0.0368 - val_loss: 0.0226\n",
            "Epoch 217/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0444 - val_loss: 0.0211\n",
            "Epoch 218/250\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0346 - val_loss: 0.0234\n",
            "Epoch 219/250\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0433 - val_loss: 0.0214\n",
            "Epoch 220/250\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.0360 - val_loss: 0.0208\n",
            "Epoch 221/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0393 - val_loss: 0.0222\n",
            "Epoch 222/250\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.0358 - val_loss: 0.0204\n",
            "Epoch 223/250\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0358 - val_loss: 0.0230\n",
            "Epoch 224/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0389 - val_loss: 0.0244\n",
            "Epoch 225/250\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0362 - val_loss: 0.0211\n",
            "Epoch 226/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0390 - val_loss: 0.0213\n",
            "Epoch 227/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0360 - val_loss: 0.0202\n",
            "Epoch 228/250\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.0360 - val_loss: 0.0245\n",
            "Epoch 229/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0393 - val_loss: 0.0226\n",
            "Epoch 230/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0379 - val_loss: 0.0221\n",
            "Epoch 231/250\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0352 - val_loss: 0.0218\n",
            "Epoch 232/250\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.0414 - val_loss: 0.0228\n",
            "Epoch 233/250\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.0368 - val_loss: 0.0218\n",
            "Epoch 234/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0388 - val_loss: 0.0208\n",
            "Epoch 235/250\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0361 - val_loss: 0.0220\n",
            "Epoch 236/250\n",
            "35/35 [==============================] - 1s 25ms/step - loss: 0.0408 - val_loss: 0.0211\n",
            "Epoch 237/250\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0351 - val_loss: 0.0220\n",
            "Epoch 238/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0446 - val_loss: 0.0208\n",
            "Epoch 239/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0318 - val_loss: 0.0203\n",
            "Epoch 240/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0393 - val_loss: 0.0211\n",
            "Epoch 241/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0373 - val_loss: 0.0216\n",
            "Epoch 242/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0369 - val_loss: 0.0204\n",
            "Epoch 243/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0355 - val_loss: 0.0237\n",
            "Epoch 244/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0414 - val_loss: 0.0213\n",
            "Epoch 245/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0340 - val_loss: 0.0200\n",
            "Epoch 246/250\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.0404 - val_loss: 0.0211\n",
            "Epoch 247/250\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0346 - val_loss: 0.0216\n",
            "Epoch 248/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0363 - val_loss: 0.0207\n",
            "Epoch 249/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0360 - val_loss: 0.0230\n",
            "Epoch 250/250\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0377 - val_loss: 0.0211\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#\n",
        "# TIME SERIES QUESTION\n",
        "#\n",
        "# Build and train a neural network to predict the time indexed variable of\n",
        "# the univariate US diesel prices (On - Highway) All types for the period of\n",
        "# 1994 - 2021.\n",
        "# Using a window of past 10 observations of 1 feature , train the model\n",
        "# to predict the next 10 observations of that feature.\n",
        "#\n",
        "# ==============================================================================\n",
        "#\n",
        "# ABOUT THE DATASET\n",
        "#\n",
        "# Original Source:\n",
        "# https://www.eia.gov/dnav/pet/pet_pri_gnd_dcus_nus_w.htm#\n",
        "#\n",
        "# For the purpose of the examination we have used the Diesel (On - Highway) -\n",
        "# All Types time series data for the period of 1994 - 2021 from the\n",
        "# aforementioned link. The dataset has 1 time indexed feature.\n",
        "# We have provided a cleaned version of the data.\n",
        "#\n",
        "# ==============================================================================\n",
        "#\n",
        "# INSTRUCTIONS\n",
        "#\n",
        "# Complete the code in following functions:\n",
        "# 1. solution_model()\n",
        "#\n",
        "# You may receive a score of 0 or your code will fail to be graded if the \n",
        "# following criteria are not met:\n",
        "#\n",
        "# 1. Model input shape must be (BATCH_SIZE, N_PAST = 10, N_FEATURES = 1),\n",
        "#    since the testing infrastructure expects a window of past N_PAST = 10\n",
        "#    observations of the 1 feature to predict the next N_FUTURE = 10\n",
        "#    observations of the same feature.\n",
        "#\n",
        "# 2. Model output shape must be (BATCH_SIZE, N_FUTURE = 10, N_FEATURES = 1)\n",
        "#\n",
        "# 3. The last layer of your model must be a Dense layer with 1 neuron since\n",
        "#    the model is expected to predict observations of 1 feature.\n",
        "#\n",
        "# 4. Don't change the values of the following constants:\n",
        "#    SPLIT_TIME, N_FEATURES, BATCH_SIZE, N_PAST, N_FUTURE, SHIFT, in\n",
        "#    solution_model() (See code for additional note on BATCH_SIZE).\n",
        "#\n",
        "# 5. Code for normalizing the data is provided - don't change it.\n",
        "#    Changing the normalizing code will affect your score.\n",
        "#\n",
        "# 6. Code for converting the dataset into windows is provided - don't change it.\n",
        "#    Changing the windowing code will affect your score.\n",
        "#\n",
        "# 7. Code for setting the seed is provided - don't change it.\n",
        "#\n",
        "# Make sure that the model architecture and input, output shapes match our\n",
        "# requirements by printing model.summary() and reviewing its output.\n",
        "#\n",
        "# HINT: If you follow all the rules mentioned above and throughout this\n",
        "# question while training your neural network, there is a possibility that a\n",
        "# validation MAE of approximately 0.02 or less on the normalized validation\n",
        "# dataset may fetch you top marks.\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# This function normalizes the dataset using min max scaling.\n",
        "# DO NOT CHANGE THIS CODE\n",
        "def normalize_series(data, min, max):\n",
        "    data = data - min\n",
        "    data = data / max\n",
        "    return data\n",
        "\n",
        "\n",
        "# This function is used to map the time series dataset into windows of\n",
        "# features and respective targets, to prepare it for training and validation.\n",
        "# The first element of the first window will be the first element of\n",
        "# the dataset.\n",
        "#\n",
        "# Consecutive windows are constructed by shifting the starting position\n",
        "# of the first window forward, one at a time (indicated by shift=1).\n",
        "#\n",
        "# For a window of n_past number of observations of the time\n",
        "# indexed variable in the dataset, the target for the window is the next\n",
        "# n_future number of observations of the variable, after the\n",
        "# end of the window.\n",
        "\n",
        "# DO NOT CHANGE THIS.\n",
        "def windowed_dataset(series, batch_size, n_past=10, n_future=10, shift=1):\n",
        "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "    ds = ds.window(size=n_past + n_future, shift=shift, drop_remainder=True)\n",
        "    ds = ds.flat_map(lambda w: w.batch(n_past + n_future))\n",
        "    ds = ds.map(lambda w: (w[:n_past], w[n_past:]))\n",
        "    return ds.batch(batch_size).prefetch(1)\n",
        "\n",
        "\n",
        "# This function loads the data from the CSV file, normalizes the data and\n",
        "# splits the dataset into train and validation data. It also uses\n",
        "# windowed_dataset() to split the data into windows of observations and\n",
        "# targets. Finally it defines, compiles and trains a neural network. This\n",
        "# function returns the final trained model.\n",
        "\n",
        "\n",
        "class OutLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super(OutLayer, self).__init__()\n",
        "    def call(self, inputs):\n",
        "        return tf.expand_dims(inputs, 2)\n",
        "\n",
        "# COMPLETE THE CODE IN THIS FUNCTION\n",
        "def solution_model():\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    # Reads the dataset.\n",
        "    df = pd.read_csv('Weekly_U.S.Diesel_Retail_Prices.csv',\n",
        "                     infer_datetime_format=True, index_col='Week of', header=0)\n",
        "\n",
        "    # Number of features in the dataset. We use all features as predictors to\n",
        "    # predict all features of future time steps.\n",
        "    N_FEATURES = len(df.columns) # DO NOT CHANGE THIS\n",
        "\n",
        "    # Normalizes the data\n",
        "    data = df.values\n",
        "    data = normalize_series(data, data.min(axis=0), data.max(axis=0))\n",
        "\n",
        "    # Splits the data into training and validation sets.\n",
        "    SPLIT_TIME = int(len(data) * 0.8) # DO NOT CHANGE THIS\n",
        "    x_train = data[:SPLIT_TIME]\n",
        "    x_valid = data[SPLIT_TIME:]\n",
        "\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    tf.keras.backend.clear_session()\n",
        "    tf.random.set_seed(42)\n",
        "\n",
        "    # DO NOT CHANGE BATCH_SIZE IF YOU ARE USING STATEFUL LSTM/RNN/GRU.\n",
        "    # THE TEST WILL FAIL TO GRADE YOUR SCORE IN SUCH CASES.\n",
        "    # In other cases, it is advised not to change the batch size since it\n",
        "    # might affect your final scores. While setting it to a lower size\n",
        "    # might not do any harm, higher sizes might affect your scores.\n",
        "    BATCH_SIZE = 32  # ADVISED NOT TO CHANGE THIS\n",
        "\n",
        "    # DO NOT CHANGE N_PAST, N_FUTURE, SHIFT. The tests will fail to run\n",
        "    # on the server.\n",
        "    # Number of past time steps based on which future observations should be\n",
        "    # predicted\n",
        "    N_PAST = 10  # DO NOT CHANGE THIS\n",
        "\n",
        "    # Number of future time steps which are to be predicted.\n",
        "    N_FUTURE = 10  # DO NOT CHANGE THIS\n",
        "\n",
        "    # By how many positions the window slides to create a new window\n",
        "    # of observations.\n",
        "    SHIFT = 1  # DO NOT CHANGE THIS\n",
        "\n",
        "    # Code to create windowed train and validation datasets.\n",
        "    train_set = windowed_dataset(series=x_train, batch_size=BATCH_SIZE,\n",
        "                                 n_past=N_PAST, n_future=N_FUTURE,\n",
        "                                 shift=SHIFT)\n",
        "    valid_set = windowed_dataset(series=x_valid, batch_size=BATCH_SIZE,\n",
        "                                 n_past=N_PAST, n_future=N_FUTURE,\n",
        "                                 shift=SHIFT)\n",
        "\n",
        "    # Code to define your model.\n",
        "    model = tf.keras.models.Sequential([\n",
        "\n",
        "        # ADD YOUR LAYERS HERE.\n",
        "        tf.keras.Input(batch_size=BATCH_SIZE, shape=(N_PAST, N_FEATURES)),\n",
        "        tf.keras.layers.Conv1D(32, 3, activation='relu', input_shape=(N_PAST, N_FEATURES)),\n",
        "        tf.keras.layers.DepthwiseConv1D(3, depth_multiplier=4, data_format='channels_last'),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dropout(0.3),\n",
        "        tf.keras.layers.Dense(320),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "        tf.keras.layers.Dense(160),\n",
        "        tf.keras.layers.Dense(80),\n",
        "        tf.keras.layers.Dense(40),\n",
        "        tf.keras.layers.Dense(20),\n",
        "        # If you don't follow the instructions in the following comments,\n",
        "        # tests will fail to grade your code:\n",
        "        # The input layer of your model must have an input shape of:\n",
        "        # (BATCH_SIZE, N_PAST = 10, N_FEATURES = 1)\n",
        "        # The model must have an output shape of:\n",
        "        # (BATCH_SIZE, N_FUTURE = 10, N_FEATURES = 1).\n",
        "        # Make sure that there are N_FEATURES = 1 neurons in the final dense\n",
        "        # layer since the model predicts 1 feature.\n",
        "\n",
        "        # HINT: Bidirectional LSTMs may help boost your score. This is only a\n",
        "        # suggestion.\n",
        "\n",
        "        # WARNING: After submitting the trained model for scoring, if you are\n",
        "        # receiving a score of 0 or an error, please recheck the input and \n",
        "        # output shapes of the model to see if it exactly matches our requirements. \n",
        "        # The grading infrastructure is very strict about the shape requirements. \n",
        "        # Most common issues occur when the shapes are not matching our \n",
        "        # expectations.\n",
        "        #\n",
        "        # TIP: You can print the output of model.summary() to review the model\n",
        "        # architecture, input and output shapes of each layer.\n",
        "        # If you have made sure that you have matched the shape requirements\n",
        "        # and all the other instructions we have laid down, and still\n",
        "        # receive a bad score, you must work on improving your model.\n",
        "\n",
        "        # WARNING: If you are using the GRU layer, it is advised not to use the\n",
        "        # recurrent_dropout argument (you can alternatively set it to 0),\n",
        "        # since it has not been implemented in the cuDNN kernel and may\n",
        "        # result in much longer training times.\n",
        "        tf.keras.layers.Dense(10),\n",
        "        OutLayer()\n",
        "    ])\n",
        "\n",
        "    # Code to train and compile the model\n",
        "    optimizer = 'adam'\n",
        "    model.compile(\n",
        "        optimizer,\n",
        "        loss='mae'\n",
        "    )\n",
        "    print(model.summary())\n",
        "    model.fit(\n",
        "        train_set,\n",
        "        batch_size = BATCH_SIZE,\n",
        "        epochs = 250,\n",
        "        validation_data = valid_set\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# Note that you'll need to save your model as a .h5 like this.\n",
        "# When you press the Submit and Test button, your saved .h5 model will\n",
        "# be sent to the testing infrastructure for scoring\n",
        "# and the score will be returned to you.\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"mymodel.h5\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mRfZtqyiXMK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Reads the dataset.\n",
        "df = pd.read_csv('Weekly_U.S.Diesel_Retail_Prices.csv',infer_datetime_format=True, index_col='Week of', header=0)\n",
        "\n",
        "    # Number of features in the dataset. We use all features as predictors to\n",
        "    # predict all features of future time steps.\n",
        "N_FEATURES = len(df.columns) # DO NOT CHANGE THIS\n",
        "\n",
        "    # Normalizes the data\n",
        "data = df.values\n",
        "data = normalize_series(data, data.min(axis=0), data.max(axis=0))\n",
        "\n",
        "    # Splits the data into training and validation sets.\n",
        "SPLIT_TIME = int(len(data) * 0.8) # DO NOT CHANGE THIS\n",
        "x_train = data[:SPLIT_TIME]\n",
        "x_valid = data[SPLIT_TIME-22:]\n",
        "print(x_train.shape, x_valid.shape)\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "tf.keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "    # DO NOT CHANGE BATCH_SIZE IF YOU ARE USING STATEFUL LSTM/RNN/GRU.\n",
        "    # THE TEST WILL FAIL TO GRADE YOUR SCORE IN SUCH CASES.\n",
        "    # In other cases, it is advised not to change the batch size since it\n",
        "    # might affect your final scores. While setting it to a lower size\n",
        "    # might not do any harm, higher sizes might affect your scores.\n",
        "BATCH_SIZE = 32  # ADVISED NOT TO CHANGE THIS\n",
        "\n",
        "    # DO NOT CHANGE N_PAST, N_FUTURE, SHIFT. The tests will fail to run\n",
        "    # on the server.\n",
        "    # Number of past time steps based on which future observations should be\n",
        "    # predicted\n",
        "N_PAST = 10  # DO NOT CHANGE THIS\n",
        "\n",
        "    # Number of future time steps which are to be predicted.\n",
        "N_FUTURE = 10  # DO NOT CHANGE THIS\n",
        "\n",
        "    # By how many positions the window slides to create a new window\n",
        "    # of observations.\n",
        "SHIFT = 1  # DO NOT CHANGE THIS\n",
        "\n",
        "    # Code to create windowed train and validation datasets.\n",
        "train_set = windowed_dataset(series=x_train, batch_size=BATCH_SIZE,\n",
        "                                 n_past=N_PAST, n_future=N_FUTURE,\n",
        "                                 shift=SHIFT)\n",
        "valid_set = windowed_dataset(series=x_valid, batch_size=BATCH_SIZE,\n",
        "                                 n_past=N_PAST, n_future=N_FUTURE,\n",
        "                                 shift=SHIFT)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Be3eXjycLomU",
        "outputId": "d432ad26-4022-4b7a-9a31-1092cf0e087d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1139, 1) (307, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# THIS CODE IS USED IN THE TESTER FOR FORECASTING. IF YOU WANT TO TEST YOUR MODEL\n",
        "# BEFORE UPLOADING YOU CAN DO IT WITH THIS\n",
        "import numpy as np\n",
        "def model_forecast(model, series, window_size, batch_size):\n",
        "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "    ds = ds.window(window_size, shift=1, drop_remainder=True)\n",
        "    ds = ds.flat_map(lambda w: w.batch(window_size))\n",
        "    ds = ds.batch(batch_size, drop_remainder=True).prefetch(1)\n",
        "    forecast = model.predict(ds)\n",
        "    return forecast\n",
        "\n",
        "# PASS THE NORMALIZED data IN THE FOLLOWING CODE\n",
        "\n",
        "rnn_forecast = model_forecast(model, data, N_PAST, BATCH_SIZE)\n",
        "rnn_forecast = rnn_forecast[SPLIT_TIME - N_PAST:-1, 0, 0]\n",
        "\n",
        "x_valid = np.squeeze(x_valid[:rnn_forecast.shape[0]])\n",
        "result = tf.keras.metrics.mean_absolute_error(x_valid, rnn_forecast).numpy()\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3Yj4S3UHk1D",
        "outputId": "608937e1-df80-4ba5-8392-6e6beeec357e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "44/44 [==============================] - 0s 7ms/step\n",
            "0.045237526\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T_z8sM5jZdFX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}