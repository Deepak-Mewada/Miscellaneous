{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN+9fbGGMup6gKInI5wzc+N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Deepak-Mewada/Miscellaneous/blob/main/sarcasm_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gh-j3VEHwycU",
        "outputId": "0942f57f-d8ee-46b7-f9aa-bfc6ff39006c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.5467 - accuracy: 0.4397\n",
            "Epoch 1: val_accuracy improved from -inf to 0.43673, saving model to mymodel.h5\n",
            "625/625 [==============================] - 87s 125ms/step - loss: 0.5467 - accuracy: 0.4397 - val_loss: 0.4514 - val_accuracy: 0.4367\n",
            "Epoch 2/30\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.3894 - accuracy: 0.4397\n",
            "Epoch 2: val_accuracy did not improve from 0.43673\n",
            "625/625 [==============================] - 80s 128ms/step - loss: 0.3894 - accuracy: 0.4397 - val_loss: 0.3993 - val_accuracy: 0.4367\n",
            "Epoch 3/30\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.3643 - accuracy: 0.4397\n",
            "Epoch 3: val_accuracy did not improve from 0.43673\n",
            "625/625 [==============================] - 78s 125ms/step - loss: 0.3643 - accuracy: 0.4397 - val_loss: 0.4100 - val_accuracy: 0.4367\n",
            "Epoch 4/30\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.3546 - accuracy: 0.4397\n",
            "Epoch 4: val_accuracy did not improve from 0.43673\n",
            "625/625 [==============================] - 76s 121ms/step - loss: 0.3546 - accuracy: 0.4397 - val_loss: 0.4109 - val_accuracy: 0.4367\n",
            "Epoch 4: early stopping\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#\n",
        "# NLP QUESTION\n",
        "#\n",
        "# Build and train a classifier for the sarcasm dataset.\n",
        "# The classifier should have a final layer with 1 neuron activated by sigmoid as shown.\n",
        "# It will be tested against a number of sentences that the network hasn't previously seen\n",
        "# and you will be scored on whether sarcasm was correctly detected in those sentences.\n",
        "\n",
        "import json\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import urllib\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "try:\n",
        "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "except:\n",
        "    pass\n",
        "\n",
        "\n",
        "def solution_model():\n",
        "    url = 'https://storage.googleapis.com/download.tensorflow.org/data/sarcasm.json'\n",
        "    urllib.request.urlretrieve(url, 'sarcasm.json')\n",
        "\n",
        "    # DO NOT CHANGE THIS CODE OR THE TESTS MAY NOT WORK\n",
        "    vocab_size = 1000\n",
        "    embedding_dim = 16\n",
        "    max_length = 120\n",
        "    trunc_type = 'post'\n",
        "    padding_type = 'post'\n",
        "    oov_tok = \"<OOV>\"\n",
        "    training_size = 20000\n",
        "\n",
        "    sentences = []\n",
        "    labels = []\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "    with open(\"sarcasm.json\", 'r') as f:\n",
        "        datastore = json.load(f)\n",
        "\n",
        "    sentences = []\n",
        "    labels = []\n",
        "\n",
        "    for item in datastore:\n",
        "        sentences.append(item['headline'])\n",
        "        labels.append(item['is_sarcastic'])\n",
        "\n",
        "    training_sentences = sentences[0:training_size]\n",
        "    testing_sentences = sentences[training_size:]\n",
        "    training_labels = labels[0:training_size]\n",
        "    testing_labels = labels[training_size:]\n",
        "\n",
        "    tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "    tokenizer.fit_on_texts(training_sentences)\n",
        "\n",
        "    word_index = tokenizer.word_index\n",
        "\n",
        "    training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
        "    training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "    testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
        "    testing_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "    callbacks = [\n",
        "        EarlyStopping(\n",
        "            monitor='val_accuracy',\n",
        "            min_delta=1e-4,\n",
        "            patience=3,\n",
        "            verbose=1\n",
        "        ),\n",
        "        ModelCheckpoint(\n",
        "            filepath='mymodel.h5',\n",
        "            monitor='val_accuracy',\n",
        "            mode='max',\n",
        "            save_best_only=True,\n",
        "            save_weights_only=False,\n",
        "            verbose=1\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    # Need this block to get it to work with TensorFlow 2.x\n",
        "    import numpy as np\n",
        "    training_padded = np.array(training_padded)\n",
        "    training_labels = np.array(training_labels)\n",
        "    testing_padded = np.array(testing_padded)\n",
        "    testing_labels = np.array(testing_labels)\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16, return_sequences=True)),\n",
        "        tf.keras.layers.GlobalAveragePooling1D(),\n",
        "        tf.keras.layers.Dense(24, activation='relu'),\n",
        "        tf.keras.layers.Dense(1, activation='softmax')\n",
        "    ])\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    num_epochs = 30\n",
        "    model.fit(training_padded, training_labels, epochs=num_epochs,\n",
        "              validation_data=(testing_padded, testing_labels), verbose=1, callbacks=callbacks)\n",
        "    return model\n",
        "\n",
        "\n",
        "# Note that you'll need to save your model as a .h5 like this.\n",
        "# When you press the Submit and Test button, your saved .h5 model will\n",
        "# be sent to the testing infrastructure for scoring\n",
        "# and the score will be returned to you.\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    # model.save(\"mymodel.bak.h5\")"
      ]
    }
  ]
}